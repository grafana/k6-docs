---
aliases:
  - ./troubleshooting # docs/k6/<K6_VERSION>/set-up/set-up-distributed-k6/troubleshooting
weight: 400
title: Troubleshoot
---

# Troubleshoot

This topic includes instructions to help you troubleshoot common issues with the k6 Operator.

If youâ€™re using Private Load Zones in Grafana Cloud k6, refer to [Troubleshoot Private Load Zones](https://grafana.com/docs/grafana-cloud/testing/k6/author-run/private-load-zone/troubleshoot/).

## How to troubleshoot

{{< docs/shared source="k6" lookup="k6-operator/troubleshooting-how-to.md" version="<K6_VERSION>" >}}

## Common errors

### Issues with environment variables

Refer to [Environment variables](https://github.com/grafana/k6-operator/blob/main/docs/env-vars.md) for details on how to pass environment variables to the k6 Operator.

### Tags not working

Tags are a rather common source of errors when using the k6 Operator. For example, the following tags would lead to parsing errors:

```yaml
  arguments: --tag product_id="Test A"
  # or
  arguments: --tag foo=\"bar\"
```

You can see those errors in the logs of either the initializer or the runner Pod, for example:

```bash
time="2024-01-11T11:11:27Z" level=error msg="invalid argument \"product_id=\\\"Test\" for \"--tag\" flag: parse error on line 1, column 12: bare \" in non-quoted-field"
```

This is a common problem with escaping the characters. You can find an [issue](https://github.com/grafana/k6-operator/issues/211) in the k6 Operator repository that can be upvoted.

### An error on reading output of the initializer Pod

The k6 runners fail to start, and in the k6 Operator logs, you see the `unable to marshal` error. This can happen for several reasons:

1. Your Kubernetes setup includes some tool that is implicitly adding symbols to the log output of Pods. You can verify this case by checking the logs of the initializer Pod: they should contain valid JSON, generated by k6. Currently, to fix this, the tool adding symbols must be switched off for the k6 Operator workloads.

2. Multi-file script includes many files which all must be fully accessible from the runner Pod. You can verify this case by checking the logs of the initializer Pod: there will be an error about some file not being found. To fix this, refer to [Multi-file tests](https://grafana.com/docs/k6/latest/set-up/set-up-distributed-k6/usage/executing-k6-scripts-with-testrun-crd/#multi-file-tests) on how to configure multi-file tests in `TestRun`.

3. There are problems with environment variables or with importing an extension. Following the steps found in [testing locally](#test-your-script-locally) can help debug this issue. One additional command that you can use to help diagnose issues with your script is the following:

```bash
k6 inspect --execution-requirements script.js
```

That command is a shortened version of what the initializer Pod is executing. If the command produces an error, there's a problem with the script itself and it should be solved outside of the k6 Operator. The error itself may contain a hint to what's wrong, such as a syntax error.

If the standalone `k6 inspect --execution-requirements` executes successfully, then it's likely a problem with `TestRun` deployment specific to your Kubernetes setup.

### An issue with `volumeClaim`

Storing k6 scripts on a persistent volume is one approach to [multi-file tests](https://grafana.com/docs/k6/latest/set-up/set-up-distributed-k6/usage/executing-k6-scripts-with-testrun-crd/#multi-file-tests). However, errors can occur due to misconfiguration of the volume. These errors are not within the purview of the k6 Operator; they are inherent to the Kubernetes setup itself, as the k6 Operator only mounts volumes to the Pods. However, here are some general recommendations to help debug such errors.

The `volumeClaim` option is expecting a persistent volume claim, so first, check the [Kubernetes documentation](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) and your infrastructure provider documentation to confirm if the volume is indeed set up correctly and can be mounted by Kubernetes pods.

Then, if the volume appears to be correct and is mounted to the k6 Pods without an issue, yet the `TestRun` fails with an error like the following:

```bash
The moduleSpecifier \"/test/utils.js\" couldn't be found on local disk.
```

This error implies that either the file was not written successfully to the Volume or there is a misconfiguration with a path. So it makes sense to create a separate debug Pod, for example, with the [`busybox` image](https://hub.docker.com/_/busybox) to confirm that the Volume contains the script and all its dependencies. Such a Pod should have a configuration similar to this one:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  volumes:
  - name: test-volume
    volumeSource:
      persistentVolumeClaim:
        claimName: test-pvc
        readOnly: false
  containers:
  - image: busybox
    name: busybox
    imagePullPolicy: IfNotPresent
    command:
      - sleep
      - "3600"
    volumeMounts:
    - mountPath: /test
      name: test-volume
  restartPolicy: Always
```

Then execute `ls /test` on this debug Pod to see which files are present.

{{< docs/shared source="k6" lookup="k6-operator/troubleshooting-common-scenarios.md" version="<K6_VERSION>" >}}
